{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cedf23d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a18b6c",
   "metadata": {},
   "source": [
    "- **랜덤포레스트(Random Forest)** 는 **Bagging**기반의 앙상블 기법중 하나로, **Boostrap Sampling**을 통해 여러개의 의사결정나무(베이스모델)를 생성한 후에 다수결 또는 평균에 따라 출력변수를 예측함\n",
    "    - **Bootstrap Sampling** : 학습 데이터로부터 원하는 크기의 샘플을 복원추출하여 샘플데이터를 구축하는 방법\n",
    "    - **Bagging** : Boostrap Aggregating의 약자로, Bootstrap Sampling을 통해 여러개의 표본을 만들고, 각 표본으로부터 베이스 모델을 형성하여 이를 결합하는 앙상블 기법\n",
    "  \n",
    "- 붓스트램 샘플링 뿐만아니라, **무작위 변수선택 기법**을 사용하여 의사결정나무의 다양성을 확보 한다.\n",
    "- **변수의 중요도**를 파악 할 수 있기때문에 공정에서 혐의인자 관리용으로 유용하게 사용 가능한 앙상블 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6452b",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b822863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib\n",
    "#한글꺠짐 방지\n",
    "matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c20fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data/class_balance.csv\",encoding=\"EUC-KR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59f8f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X466</th>\n",
       "      <th>X467</th>\n",
       "      <th>X468</th>\n",
       "      <th>X469</th>\n",
       "      <th>X470</th>\n",
       "      <th>X471</th>\n",
       "      <th>X472</th>\n",
       "      <th>X473</th>\n",
       "      <th>X474</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457896</td>\n",
       "      <td>0.530189</td>\n",
       "      <td>0.276976</td>\n",
       "      <td>0.359864</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.553781</td>\n",
       "      <td>0.653894</td>\n",
       "      <td>0.375204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.402240</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>0.274876</td>\n",
       "      <td>0.210238</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.341478</td>\n",
       "      <td>0.518992</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.469654</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.542031</td>\n",
       "      <td>0.447466</td>\n",
       "      <td>0.189233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523785</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029759</td>\n",
       "      <td>0.210356</td>\n",
       "      <td>0.309339</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.360781</td>\n",
       "      <td>0.369653</td>\n",
       "      <td>0.341039</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.181737</td>\n",
       "      <td>0.528684</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.516722</td>\n",
       "      <td>0.300371</td>\n",
       "      <td>0.376835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185769</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.460910</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.561615</td>\n",
       "      <td>0.415328</td>\n",
       "      <td>0.313214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263068</td>\n",
       "      <td>0.279821</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.638747</td>\n",
       "      <td>0.660074</td>\n",
       "      <td>0.520392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110711</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.608414</td>\n",
       "      <td>0.212062</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.360022</td>\n",
       "      <td>0.396033</td>\n",
       "      <td>0.382803</td>\n",
       "      <td>0.070771</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>0.920884</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.636336</td>\n",
       "      <td>0.337454</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015997</td>\n",
       "      <td>0.045307</td>\n",
       "      <td>0.147860</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.508628</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.192378</td>\n",
       "      <td>0.061866</td>\n",
       "      <td>0.168425</td>\n",
       "      <td>0.481919</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.270563</td>\n",
       "      <td>0.407911</td>\n",
       "      <td>0.336052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132582</td>\n",
       "      <td>0.975694</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.111165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.275930</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>0.302236</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.485135</td>\n",
       "      <td>0.627270</td>\n",
       "      <td>0.594828</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.420272</td>\n",
       "      <td>0.367047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.143012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.785179</td>\n",
       "      <td>0.271804</td>\n",
       "      <td>0.400189</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.229526</td>\n",
       "      <td>0.244320</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.875565</td>\n",
       "      <td>0.110012</td>\n",
       "      <td>0.337684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221768</td>\n",
       "      <td>0.767361</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.509709</td>\n",
       "      <td>0.398833</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.571191</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.401357</td>\n",
       "      <td>0.249864</td>\n",
       "      <td>0.229873</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.592648</td>\n",
       "      <td>0.498146</td>\n",
       "      <td>0.340946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214622</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>0.020904</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>0.309061</td>\n",
       "      <td>0.178988</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>0.179953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.457896  0.530189  0.276976  0.359864  0.193059  0.322190  0.706897   \n",
       "1    0.607100  0.341478  0.518992  0.395300  0.042071  0.469654  0.750000   \n",
       "2    0.360781  0.369653  0.341039  0.021697  0.181737  0.528684  0.491379   \n",
       "3    0.460910  0.413500  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "4    0.263068  0.279821  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "249  0.360022  0.396033  0.382803  0.070771  0.143308  0.920884  0.586207   \n",
       "250  0.508628  0.437628  0.192378  0.061866  0.168425  0.481919  0.715517   \n",
       "251  0.275930  0.364365  0.302236  0.376615  0.485135  0.627270  0.594828   \n",
       "252  0.785179  0.271804  0.400189  0.457851  0.229526  0.244320  0.396552   \n",
       "253  0.164343  0.571191  0.324472  0.401357  0.249864  0.229873  0.448276   \n",
       "\n",
       "           X8        X9       X10  ...      X466      X467      X468  \\\n",
       "0    0.553781  0.653894  0.375204  ...  0.246376  0.711806  0.008532   \n",
       "1    0.542031  0.447466  0.189233  ...  0.523785  0.760417  0.030930   \n",
       "2    0.516722  0.300371  0.376835  ...  0.185769  0.659722  0.005333   \n",
       "3    0.561615  0.415328  0.313214  ...  0.246376  0.000000  1.000000   \n",
       "4    0.638747  0.660074  0.520392  ...  0.110711  0.517361  0.023677   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "249  0.636336  0.337454  0.432300  ...  0.000000  0.597222  0.016212   \n",
       "250  0.270563  0.407911  0.336052  ...  0.132582  0.975694  0.017065   \n",
       "251  0.435673  0.420272  0.367047  ...  0.246376  0.586806  0.008532   \n",
       "252  0.875565  0.110012  0.337684  ...  0.221768  0.767361  0.024317   \n",
       "253  0.592648  0.498146  0.340946  ...  0.214622  0.621528  0.020904   \n",
       "\n",
       "         X469      X470      X471      X472      X473      X474  Y  \n",
       "0    0.013672  0.008467  0.402240  0.238811  0.274876  0.210238 -1  \n",
       "1    0.033203  0.029759  0.210356  0.309339  0.328358  0.439175 -1  \n",
       "2    0.003906  0.005311  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "3    1.000000  1.000000  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "4    0.022461  0.023447  0.608414  0.212062  0.268657  0.092827 -1  \n",
       "..        ...       ...       ...       ...       ...       ... ..  \n",
       "249  0.013672  0.015997  0.045307  0.147860  0.171642  0.000000  1  \n",
       "250  0.015625  0.016114  0.543689  0.227626  0.320896  0.111165  1  \n",
       "251  0.015625  0.008613  0.343042  0.151751  0.164179  0.143012  1  \n",
       "252  0.019531  0.023461  0.509709  0.398833  0.440299  0.185945  1  \n",
       "253  0.018555  0.020352  0.309061  0.178988  0.201493  0.179953  1  \n",
       "\n",
       "[254 rows x 475 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aca0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y 분할\n",
    "Y=data[\"Y\"].copy()\n",
    "X=data.drop(\"Y\",axis=1)\n",
    "X.head(3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2,shuffle =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdb2ae",
   "metadata": {},
   "source": [
    "[[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)]  \n",
    "**sklearn.model_selection.train_test_split**\n",
    "- **test_size** : float or int, default = 0.25, 정수값일시 test사이즈로 설정하고 싶은 샘플 수 입력\n",
    "- **train_size** : float or int, default = None\n",
    "- **random_state** : int, default = None, 랜덤 seed값 설정, 같은 seed 내에선 동일결과 추출 \n",
    "- **shuffle** : bool, default = True, 데이터셋 무작위 추출, 시계열 데이터와 같이 순차적 추출이 필요한 경우엔 Shuffle = False!\n",
    "- **stratify** : array-like, default = None, True일시 계층적 샘플링 진행 ([참고](https://www.investopedia.com/terms/stratified_random_sampling.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a71bc",
   "metadata": {},
   "source": [
    "### 2. 평가 지표 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544404b",
   "metadata": {},
   "source": [
    "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)\n",
    "\n",
    "###### 이미지 출처 : https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3227a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 출력 함수\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_all_reg(Y_test,pred):\n",
    "    # Specificity를 구하기 위해 confusion matrix를 이용\n",
    "    cm1 = confusion_matrix(Y_test,pred)\n",
    "    specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    \n",
    "    #결과 검사\n",
    "    #recall = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "    #pre = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "\n",
    "    G_mean = recall_score(Y_test,pred) * specificity1\n",
    "    \n",
    "    print(\"model의 recall 값은 {:.3f}\".format(recall_score(Y_test,pred)))\n",
    "    print(\"model의 2종 오류 확률 값은 {:.3f}\".format(1-recall_score(Y_test,pred)))\n",
    "    print(\"model의 Specificity 값은 {:.3f}\".format(specificity1))\n",
    "    print(\"model의 1종 오류 확률 값은 {:.3f}\".format(1-specificity1))\n",
    "    print(\"model의 precision 값은 {:.3f}\".format(precision_score(Y_test,pred)))\n",
    "    print(\"model의 f1_score 값은 {:.3f}\".format(f1_score(Y_test,pred)))\n",
    "    print(\"model의 G-mean 값은 {:.3f}\".format(np.sqrt(G_mean)))\n",
    "    print(\"model의 accuracy 값은 {:.3f}\".format(accuracy_score(Y_test,pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ec9ce",
   "metadata": {},
   "source": [
    "### 3. 모델 학습 및 예측 (하이퍼 파라미터 default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e52025",
   "metadata": {},
   "source": [
    "[[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)]         \n",
    "**주요 하이퍼파라미터** \n",
    "- **n_estimators** : int, default=100, 랜덤포레스트를 구성하는 의사결정 나무 개수 \n",
    "- **criterion** : {\"gini\", \"entorpy\", \"log_loss\"}, default = \"gini\", 클래스 동질성을 측정하는 지표 설정, CART에서는 지니불순도(Gini Impurity)를 사용함.\n",
    "- **max_depth** : int, default = None, 각 트리의 최대깊이를 설정. 값이 클수록 모델의 복잡도가 올라간다.\n",
    "- **min_samples_split** : int or float, default = 2, 자식노드를 분할하는데 필요한 최소 샘플의 수\n",
    "- **min_samples_leaf** : int or float, default = 1, leaf node에서 필요한 최소 샘풀수이며, 너무 적을 시 과적합 발생\n",
    "- **max_leaf_nodes** : int, default=None, 최대 leaf node 수 제한\n",
    "- **max_features** : int, float or {“auto”, “sqrt”, “log2”}, default=sqrt, 각 노드를 분리할 때 사용 할 최대 속성 수\n",
    "- **bootstrap** : bool, default = True, Tree를 형성할때 붓스트램 샘플을 활용. False일시, 중복을 허용하지 않음(=pasting)\n",
    "- **n_jobs** : int, deafult = None, 학습과정에서 사용할 컴퓨터 코어 수, -1로 설정시 가능한 모든 컴퓨터 코어 사용\n",
    "- **random_state** : int, deafult = None, 랜덤값을 지정함으로써 동일결과 출력되도록 설정. default로 사용 시 매번 다른 forest가 형성되나 n_estimators 값이 커질수록 그 변동성은 작아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a34ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 recall 값은 1.000\n",
      "model의 2종 오류 확률 값은 0.000\n",
      "model의 Specificity 값은 1.000\n",
      "model의 1종 오류 확률 값은 0.000\n",
      "model의 precision 값은 1.000\n",
      "model의 f1_score 값은 1.000\n",
      "model의 G-mean 값은 1.000\n",
      "model의 accuracy 값은 1.000\n",
      "\n",
      "model의 recall 값은 0.733\n",
      "model의 2종 오류 확률 값은 0.267\n",
      "model의 Specificity 값은 0.979\n",
      "model의 1종 오류 확률 값은 0.021\n",
      "model의 precision 값은 0.957\n",
      "model의 f1_score 값은 0.830\n",
      "model의 G-mean 값은 0.847\n",
      "model의 accuracy 값은 0.883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model=RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rfc_model.fit(X_train,Y_train)\n",
    "\n",
    "rfc_pred_train = rfc_model.predict(X_train) # 훈련데이터를 활용하여, 학습한 모델로 예측값 생성\n",
    "rfc_pred=rfc_model.predict(X_test) # 테스트 데이터를 활용하여, 학습한 모델로 예측값 생성\n",
    "\n",
    "print_all_reg(Y_train,rfc_pred_train) # 훈련데이터셋을 활용한 모델 성능 평가\n",
    "print(\"\")\n",
    "print_all_reg(Y_test,rfc_pred) # 테스트 데이터셋을 활용한 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "731e972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# 데이터 전처리 및 랜덤포레스트 모델 학습과정을 자동화 하는 함수\n",
    "def auto_randomforest(data_x,data_y,model,scaler_how,params):\n",
    "    \n",
    "    # 데이터 split\n",
    "    split_num = 5 # k값 설정\n",
    "    kfold = KFold(n_splits=split_num, shuffle=True, random_state=10) # K-fold 교차검증 적용\n",
    "    score = []\n",
    "    for train_index,test_index in kfold.split(data_x): # k-fold 결과 인덱스가 반환되어 데이터 분할 가능\n",
    "        train_x,valid_x = data_x.iloc[train_index],data_x.iloc[test_index] # data_x를 훈련용, 검증용 데이터셋으로 분할\n",
    "        train_y,valid_y = data_y.iloc[train_index],data_y.iloc[test_index] # data_y를 훈련용, 검증용 데이터셋으로 분할\n",
    "        \n",
    "        # 1 .model select \n",
    "        if model == \"rf\":\n",
    "            \n",
    "            #2. params를 통해 하이퍼 파라미터 세팅\n",
    "            model_use = RandomForestClassifier(random_state=0, n_estimators = params['n_estimators'],\n",
    "                                               max_depth = params[\"max_depth\"],min_samples_leaf = params[\"min_samples_leaf\"],\n",
    "                                               min_samples_split = params[\"min_samples_split\"]\n",
    "                                              )      \n",
    "        else:\n",
    "            print(\"모델이 존재하지 않습니다.\")\n",
    "            break\n",
    "        \n",
    "        # 3. Scaling \n",
    "        if scaler_how == \"minmax\":\n",
    "            print(\"minmax\")\n",
    "            scaler = MinMaxScaler()\n",
    "            train_x = scaler.fit_transform(train_x) # 훈련용 데이터 MinMax Scaler 학습\n",
    "            valid_x = scaler.transform(valid_x) # 앞서 학습한 Scaler로 검증용 데이터 스케일링, 검증 데이터셋에는 fit_transform 하지 않도록 주의\n",
    "        elif scaler_how == \"zscore\":\n",
    "            print(\"z변환\")\n",
    "            scaler2 = StandardScaler()\n",
    "            train_x = scaler2.fit_transform(train_x) # 훈련용 데이터 Standard Scaler 학습\n",
    "            valid_x = scaler2.transform(valid_x) # 앞서 학습한 Scaler로 검증용 데이터 스케일링 검증 데이터셋에는 fit_transform 하지 않도록 주의     \n",
    "        else:\n",
    "            print(\"해당하는 스케일러가 없습니다.\")\n",
    "            #print(\"스케일링 변환 X\")\n",
    "            \n",
    "        # 4. FIT & TEST\n",
    "        model_use.fit(train_x, train_y) # 훈련용 데이터로 랜덤포레스트 모델학습\n",
    "        valid_pred = model_use.predict(valid_x) # 검증용데이터로 예측값 생성\n",
    "        tem = f1_score(valid_pred,valid_y) # 성능평가\n",
    "        score.append(tem) # 결과점수 저장, 해당 과정을 k번 반복\n",
    "    total_score = np.mean(score) # k번 검증된 결과의 평균 출력\n",
    "    print(\"%s 모델 K-fold 결과 f1_score: %f \"%(model,total_score))\n",
    "\n",
    "    return total_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eae864",
   "metadata": {},
   "source": [
    "참고 : [K-fold 교차검증](https://jonsyou.tistory.com/23), [fit_transform과 transform의 차이](https://deepinsight.tistory.com/165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc1a5e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.755844 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.763463 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.755844 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.763463 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.747879 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.761222 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.736169 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.736169 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.736169 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.748918 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.748918 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722987 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.731263 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.737060 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.731263 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.743384 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.716601 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.735108 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.722134 \n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "z변환\n",
      "rf 모델 K-fold 결과 f1_score: 0.728801 \n",
      "BEST SCORE 0.7634632034632034\n",
      "BEST PARAMS {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# 원하는 파라미터 설정 (파라미터를 추가하려면 위의 함수에도 추가해줘야함.)\n",
    "params={\n",
    "    \"n_estimators\":[500,1000],\n",
    "    \"max_depth\":[4,6,8,10],\n",
    "    \"min_samples_leaf\" : [2, 4, 6],\n",
    "    \"min_samples_split\" : range(2,8,2)\n",
    "}\n",
    "\n",
    "# ParameterGrid 통해서 모든 경우의 수 만들기 \n",
    "params_list = list(ParameterGrid(params)) # 총 72가지 경우의수가 담긴 리스트 반환\n",
    "\n",
    "# Grid search 진행\n",
    "score_list = []\n",
    "for params2 in params_list:\n",
    "    tem = auto_randomforest(X_train,Y_train,'rf',\"zscore\",params2) # StandardScaler를 적용하여 랜덤포레스트 모델 학습\n",
    "    score_list.append(tem)\n",
    "\n",
    "# BEST SCORE 계산\n",
    "best_index= np.argmax(score_list)\n",
    "print(\"BEST SCORE\", score_list[best_index])\n",
    "print(\"BEST PARAMS\", params_list[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3363219",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17dc3908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 recall 값은 0.824\n",
      "model의 2종 오류 확률 값은 0.176\n",
      "model의 Specificity 값은 1.000\n",
      "model의 1종 오류 확률 값은 0.000\n",
      "model의 precision 값은 1.000\n",
      "model의 f1_score 값은 0.904\n",
      "model의 G-mean 값은 0.908\n",
      "model의 accuracy 값은 0.927\n",
      " \n",
      "model의 recall 값은 0.667\n",
      "model의 2종 오류 확률 값은 0.333\n",
      "model의 Specificity 값은 0.979\n",
      "model의 1종 오류 확률 값은 0.021\n",
      "model의 precision 값은 0.952\n",
      "model의 f1_score 값은 0.784\n",
      "model의 G-mean 값은 0.808\n",
      "model의 accuracy 값은 0.857\n"
     ]
    }
   ],
   "source": [
    "# 최적의 하이퍼 파라미터로 모델학습\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators = 500,\n",
    "                                               ccp_alpha = 0.04,\n",
    "                                              )  \n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "preds_train = clf.predict(X_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print_all_reg(Y_train,preds_train)\n",
    "print(\" \")\n",
    "print_all_reg(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7f53df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#주요 변수 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "var_imp = pd.DataFrame({'var_name':X.columns , 'var_importance': clf.feature_importances_}) # 변수명과 해당 변수의 중요도 값을 데이터프레임 형태로 생성\n",
    "imp_top20=var_imp.sort_values(by=['var_importance'],ascending=False)[:20] # 결과값 내림차순 정렬\n",
    "imp_top20.head(n=10) # 상위값 10개만 출력\n",
    "plt.figure\n",
    "plt.title(\"Feature importaces Top 20\")\n",
    "sns.barplot(x=imp_top20[\"var_importance\"], y=imp_top20[\"var_name\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e81d91ba51e1bf9900dd7d036cbe3d31d033e3ae1051184964e8f8743fed6bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
