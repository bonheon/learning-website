{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2146817",
   "metadata": {},
   "source": [
    "# 의사결정나무(Decison Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155a1d5",
   "metadata": {},
   "source": [
    "#### **사후 가지치기**\n",
    ": 나무가 완성된 후에 하단 노드부터 유의미하지 않다고 판단되는 Subtree를 끝노드로 변환시키는 방법    \n",
    "\n",
    "#### [Cost Complexity Pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)    \n",
    "$R_\\alpha(T) = R(T) + \\alpha\\vert T \\vert$\n",
    "\n",
    "  - 위의 비용복잡도(Cost complexity) 식을 최소화 시키는 과정(CCP)\n",
    "  - Weakest link pruning이라고도 한다.\n",
    "  - 가지가 있을 때와 처낼 때 불순도의 차이가 거의 없는 가지(Weakest Link)를 가지치기하는 메커니즘\n",
    "  - 모든 Subtree를 고려하진 않아 Local Minima에 빠질 수 있다.\n",
    "\n",
    "    \n",
    "참고 : https://zephyrus1111.tistory.com/131\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6452b",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib\n",
    "#한글꺠짐 방지\n",
    "matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c20fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data/class_balance.csv\",encoding=\"EUC-KR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59f8f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X466</th>\n",
       "      <th>X467</th>\n",
       "      <th>X468</th>\n",
       "      <th>X469</th>\n",
       "      <th>X470</th>\n",
       "      <th>X471</th>\n",
       "      <th>X472</th>\n",
       "      <th>X473</th>\n",
       "      <th>X474</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457896</td>\n",
       "      <td>0.530189</td>\n",
       "      <td>0.276976</td>\n",
       "      <td>0.359864</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.553781</td>\n",
       "      <td>0.653894</td>\n",
       "      <td>0.375204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.402240</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>0.274876</td>\n",
       "      <td>0.210238</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.341478</td>\n",
       "      <td>0.518992</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.469654</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.542031</td>\n",
       "      <td>0.447466</td>\n",
       "      <td>0.189233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523785</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029759</td>\n",
       "      <td>0.210356</td>\n",
       "      <td>0.309339</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.360781</td>\n",
       "      <td>0.369653</td>\n",
       "      <td>0.341039</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.181737</td>\n",
       "      <td>0.528684</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.516722</td>\n",
       "      <td>0.300371</td>\n",
       "      <td>0.376835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185769</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.460910</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.561615</td>\n",
       "      <td>0.415328</td>\n",
       "      <td>0.313214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263068</td>\n",
       "      <td>0.279821</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.638747</td>\n",
       "      <td>0.660074</td>\n",
       "      <td>0.520392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110711</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.608414</td>\n",
       "      <td>0.212062</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.360022</td>\n",
       "      <td>0.396033</td>\n",
       "      <td>0.382803</td>\n",
       "      <td>0.070771</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>0.920884</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.636336</td>\n",
       "      <td>0.337454</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015997</td>\n",
       "      <td>0.045307</td>\n",
       "      <td>0.147860</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.508628</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.192378</td>\n",
       "      <td>0.061866</td>\n",
       "      <td>0.168425</td>\n",
       "      <td>0.481919</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.270563</td>\n",
       "      <td>0.407911</td>\n",
       "      <td>0.336052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132582</td>\n",
       "      <td>0.975694</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.111165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.275930</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>0.302236</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.485135</td>\n",
       "      <td>0.627270</td>\n",
       "      <td>0.594828</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.420272</td>\n",
       "      <td>0.367047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.143012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.785179</td>\n",
       "      <td>0.271804</td>\n",
       "      <td>0.400189</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.229526</td>\n",
       "      <td>0.244320</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.875565</td>\n",
       "      <td>0.110012</td>\n",
       "      <td>0.337684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221768</td>\n",
       "      <td>0.767361</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.509709</td>\n",
       "      <td>0.398833</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.571191</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.401357</td>\n",
       "      <td>0.249864</td>\n",
       "      <td>0.229873</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.592648</td>\n",
       "      <td>0.498146</td>\n",
       "      <td>0.340946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214622</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>0.020904</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>0.309061</td>\n",
       "      <td>0.178988</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>0.179953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.457896  0.530189  0.276976  0.359864  0.193059  0.322190  0.706897   \n",
       "1    0.607100  0.341478  0.518992  0.395300  0.042071  0.469654  0.750000   \n",
       "2    0.360781  0.369653  0.341039  0.021697  0.181737  0.528684  0.491379   \n",
       "3    0.460910  0.413500  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "4    0.263068  0.279821  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "249  0.360022  0.396033  0.382803  0.070771  0.143308  0.920884  0.586207   \n",
       "250  0.508628  0.437628  0.192378  0.061866  0.168425  0.481919  0.715517   \n",
       "251  0.275930  0.364365  0.302236  0.376615  0.485135  0.627270  0.594828   \n",
       "252  0.785179  0.271804  0.400189  0.457851  0.229526  0.244320  0.396552   \n",
       "253  0.164343  0.571191  0.324472  0.401357  0.249864  0.229873  0.448276   \n",
       "\n",
       "           X8        X9       X10  ...      X466      X467      X468  \\\n",
       "0    0.553781  0.653894  0.375204  ...  0.246376  0.711806  0.008532   \n",
       "1    0.542031  0.447466  0.189233  ...  0.523785  0.760417  0.030930   \n",
       "2    0.516722  0.300371  0.376835  ...  0.185769  0.659722  0.005333   \n",
       "3    0.561615  0.415328  0.313214  ...  0.246376  0.000000  1.000000   \n",
       "4    0.638747  0.660074  0.520392  ...  0.110711  0.517361  0.023677   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "249  0.636336  0.337454  0.432300  ...  0.000000  0.597222  0.016212   \n",
       "250  0.270563  0.407911  0.336052  ...  0.132582  0.975694  0.017065   \n",
       "251  0.435673  0.420272  0.367047  ...  0.246376  0.586806  0.008532   \n",
       "252  0.875565  0.110012  0.337684  ...  0.221768  0.767361  0.024317   \n",
       "253  0.592648  0.498146  0.340946  ...  0.214622  0.621528  0.020904   \n",
       "\n",
       "         X469      X470      X471      X472      X473      X474  Y  \n",
       "0    0.013672  0.008467  0.402240  0.238811  0.274876  0.210238 -1  \n",
       "1    0.033203  0.029759  0.210356  0.309339  0.328358  0.439175 -1  \n",
       "2    0.003906  0.005311  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "3    1.000000  1.000000  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "4    0.022461  0.023447  0.608414  0.212062  0.268657  0.092827 -1  \n",
       "..        ...       ...       ...       ...       ...       ... ..  \n",
       "249  0.013672  0.015997  0.045307  0.147860  0.171642  0.000000  1  \n",
       "250  0.015625  0.016114  0.543689  0.227626  0.320896  0.111165  1  \n",
       "251  0.015625  0.008613  0.343042  0.151751  0.164179  0.143012  1  \n",
       "252  0.019531  0.023461  0.509709  0.398833  0.440299  0.185945  1  \n",
       "253  0.018555  0.020352  0.309061  0.178988  0.201493  0.179953  1  \n",
       "\n",
       "[254 rows x 475 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aca0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y 분할\n",
    "Y=data[\"Y\"].copy()\n",
    "X=data.drop(\"Y\",axis=1)\n",
    "X.head(3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=22,shuffle =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092e8a5",
   "metadata": {},
   "source": [
    "[[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)]  \n",
    "**sklearn.model_selection.train_test_split**\n",
    "- **test_size** : float or int, default = 0.25, 정수값일시 test사이즈로 설정하고 싶은 샘플 수 입력\n",
    "- **train_size** : float or int, default = None\n",
    "- **random_state** : int, default = None, 랜덤 seed값 설정, 같은 seed 내에선 동일결과 추출 \n",
    "- **shuffle** : bool, default = True, 데이터셋 무작위 추출, 시계열 데이터와 같이 순차적 추출이 필요한 경우엔 Shuffle = False!\n",
    "- **stratify** : array-like, default = None, True일시 계층적 샘플링 진행 ([참고](https://www.investopedia.com/terms/stratified_random_sampling.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a71bc",
   "metadata": {},
   "source": [
    "### 2. 평가 지표 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915b02e",
   "metadata": {},
   "source": [
    "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)\n",
    "\n",
    "###### 이미지 출처 : https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3227a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 출력 함수\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_all_reg(Y_test,pred):\n",
    "    # Specificity를 구하기 위해 confusion matrix를 이용\n",
    "    cm1 = confusion_matrix(Y_test,pred)\n",
    "    specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    \n",
    "    #결과 검사\n",
    "    #recall = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "    #pre = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "\n",
    "    G_mean = recall_score(Y_test,pred) * specificity1\n",
    "    \n",
    "    print(\"model의 recall 값은 {:.3f}\".format(recall_score(Y_test,pred)))\n",
    "    print(\"model의 2종 오류 확률 값은 {:.3f}\".format(1-recall_score(Y_test,pred)))\n",
    "    print(\"model의 Specificity 값은 {:.3f}\".format(specificity1))\n",
    "    print(\"model의 1종 오류 확률 값은 {:.3f}\".format(1-specificity1))\n",
    "    print(\"model의 precision 값은 {:.3f}\".format(precision_score(Y_test,pred)))\n",
    "    print(\"model의 f1_score 값은 {:.3f}\".format(f1_score(Y_test,pred)))\n",
    "    print(\"model의 G-mean 값은 {:.3f}\".format(np.sqrt(G_mean)))\n",
    "    print(\"model의 accuracy 값은 {:.3f}\".format(accuracy_score(Y_test,pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ec9ce",
   "metadata": {},
   "source": [
    "### 3. 모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a34ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ccp_alpha_list = list(np.arange(0.25,0,-0.001)) # 0.25부터 0까지 0.001씩 감소\n",
    "ccp_alpha_list2 = list(np.arange(0,0.25,0.001)) # 0부터 0.25까지 0.001씩 증가\n",
    "\n",
    "#ccp_alpha_list = np.logspace(-10, 0, num=200)\n",
    "#ccp_alpha_list2 = np.logspace(-10, 0, num=200)\n",
    "train_scores =[]\n",
    "test_scores =[]\n",
    "for alpha in ccp_alpha_list:\n",
    "    clf = DecisionTreeClassifier(ccp_alpha= alpha) # ccp_alpha 값보다 작으면서 비용복잡도가 가장 큰 Subtree\n",
    "    clf.fit(X_train,Y_train) # 학습용 데이터로 모델 학습\n",
    "    \n",
    "    preds_train = clf.predict(X_train) # 모델 예측\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    train_scores.append(1-accuracy_score(Y_train,preds_train)) # 학습용 데이터 오분류율\n",
    "    test_scores.append(1-accuracy_score(Y_test,preds)) # 테스트 데이터 오분류율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1a5e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ccp-alpha 값을 변화시키면서 얻은 모델 결과 시각화\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"ccp alpha\")\n",
    "ax.set_ylabel(\"1-accuracy\")\n",
    "#ax.set_title(\"나무 복잡도 증가에 따른 오분류율 그래프\")\n",
    "ax.plot(ccp_alpha_list, pd.Series(train_scores).rolling(10,center =True).mean(),  label=\"train data\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alpha_list, pd.Series(test_scores).rolling(10,center=True).mean(),  label=\"test data\", drawstyle=\"steps-post\")\n",
    "\n",
    "#ax.plot(ccp_alpha_list, train_scores, drawstyle=\"steps-post\")\n",
    "#ax.plot(ccp_alpha_list, test_scores, drawstyle=\"steps-post\")\n",
    "\n",
    "ax.legend()\n",
    "#plt.xlim(0.7)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e90ad7",
   "metadata": {},
   "source": [
    "- 테스트 데이터에서 오분류율이 가장 낮은 alpha = 0.02를 최적값으로 선택!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 ccp-alpha로 모델 학습\n",
    "\n",
    "alpha = 0.02\n",
    "clf = DecisionTreeClassifier(ccp_alpha= alpha)  \n",
    "clf.fit(X_train,Y_train)\n",
    "preds = clf.predict(X_test)\n",
    "preds_train = clf.predict(X_train) # 훈련용 X 데이터셋으로 예측값 생성\n",
    "print_all_reg(Y_train,preds_train) # 실제 y값과 예측값을 비교하여 성능지표 출력\n",
    "print(\" \")\n",
    "print_all_reg(Y_test,preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e81d91ba51e1bf9900dd7d036cbe3d31d033e3ae1051184964e8f8743fed6bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
