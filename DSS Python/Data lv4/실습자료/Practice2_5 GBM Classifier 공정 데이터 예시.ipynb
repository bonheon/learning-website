{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d181c0f7",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine(GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e28d8c",
   "metadata": {},
   "source": [
    "- GBM은 𝑚라운드의 베이스 모델 $ℎ_𝑚$ 이 $(m-1)$라운드까지의 베이스 모델을 결합한 $𝐻𝑚−1=ℎ_1+⋯+ℎ_{𝑚−1}$의 예측 오류를 학습하는 알고리즘\n",
    "- 즉, 이전 결합 모델 $H_{m-1}$의 예측 오류는 예측 불가능한 랜덤 노이즈가 아닌 아직까지 학습 못한 특징이라고 가정함\n",
    "- 아래는 GBM의 예시로, $ℎ_2$는 $ℎ_1$의 잔차 $(𝑦-\\hat{𝑦_1})$를 , $ℎ_3$는 $𝐻_2=ℎ_1+ℎ_2$의 잔차 $(𝑦-\\hat{𝑦_1}-\\hat{𝑦_2})$ 를 학습하고 최종적으로 $ℎ_1,ℎ_2,ℎ_3$를 결합하여 강한 모델 $𝐻_3=ℎ_1+ℎ_2+ℎ_3=𝐻_2+ℎ_3$를 만듦  \n",
    "- 분류문제에서 GBM은 로지스틱 회귀처럼 성공일 확률을 예측함(성공을 불량으로 간주하면 불량 확률을 예측) \n",
    "  \n",
    "\n",
    "![gbm](img\\부스팅.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6452b",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib\n",
    "#한글꺠짐 방지\n",
    "matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c20fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data/class_balance.csv\",encoding=\"EUC-KR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59f8f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X466</th>\n",
       "      <th>X467</th>\n",
       "      <th>X468</th>\n",
       "      <th>X469</th>\n",
       "      <th>X470</th>\n",
       "      <th>X471</th>\n",
       "      <th>X472</th>\n",
       "      <th>X473</th>\n",
       "      <th>X474</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457896</td>\n",
       "      <td>0.530189</td>\n",
       "      <td>0.276976</td>\n",
       "      <td>0.359864</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.553781</td>\n",
       "      <td>0.653894</td>\n",
       "      <td>0.375204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.402240</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>0.274876</td>\n",
       "      <td>0.210238</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.341478</td>\n",
       "      <td>0.518992</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.469654</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.542031</td>\n",
       "      <td>0.447466</td>\n",
       "      <td>0.189233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523785</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029759</td>\n",
       "      <td>0.210356</td>\n",
       "      <td>0.309339</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.360781</td>\n",
       "      <td>0.369653</td>\n",
       "      <td>0.341039</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.181737</td>\n",
       "      <td>0.528684</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.516722</td>\n",
       "      <td>0.300371</td>\n",
       "      <td>0.376835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185769</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.460910</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.561615</td>\n",
       "      <td>0.415328</td>\n",
       "      <td>0.313214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381877</td>\n",
       "      <td>0.208171</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.155761</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263068</td>\n",
       "      <td>0.279821</td>\n",
       "      <td>0.535685</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.242326</td>\n",
       "      <td>0.408966</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.638747</td>\n",
       "      <td>0.660074</td>\n",
       "      <td>0.520392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110711</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.608414</td>\n",
       "      <td>0.212062</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.360022</td>\n",
       "      <td>0.396033</td>\n",
       "      <td>0.382803</td>\n",
       "      <td>0.070771</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>0.920884</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.636336</td>\n",
       "      <td>0.337454</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015997</td>\n",
       "      <td>0.045307</td>\n",
       "      <td>0.147860</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.508628</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.192378</td>\n",
       "      <td>0.061866</td>\n",
       "      <td>0.168425</td>\n",
       "      <td>0.481919</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.270563</td>\n",
       "      <td>0.407911</td>\n",
       "      <td>0.336052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132582</td>\n",
       "      <td>0.975694</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.111165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.275930</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>0.302236</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.485135</td>\n",
       "      <td>0.627270</td>\n",
       "      <td>0.594828</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.420272</td>\n",
       "      <td>0.367047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.143012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.785179</td>\n",
       "      <td>0.271804</td>\n",
       "      <td>0.400189</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.229526</td>\n",
       "      <td>0.244320</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.875565</td>\n",
       "      <td>0.110012</td>\n",
       "      <td>0.337684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221768</td>\n",
       "      <td>0.767361</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.509709</td>\n",
       "      <td>0.398833</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.571191</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.401357</td>\n",
       "      <td>0.249864</td>\n",
       "      <td>0.229873</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.592648</td>\n",
       "      <td>0.498146</td>\n",
       "      <td>0.340946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214622</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>0.020904</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>0.309061</td>\n",
       "      <td>0.178988</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>0.179953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.457896  0.530189  0.276976  0.359864  0.193059  0.322190  0.706897   \n",
       "1    0.607100  0.341478  0.518992  0.395300  0.042071  0.469654  0.750000   \n",
       "2    0.360781  0.369653  0.341039  0.021697  0.181737  0.528684  0.491379   \n",
       "3    0.460910  0.413500  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "4    0.263068  0.279821  0.535685  0.302794  0.242326  0.408966  0.646552   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "249  0.360022  0.396033  0.382803  0.070771  0.143308  0.920884  0.586207   \n",
       "250  0.508628  0.437628  0.192378  0.061866  0.168425  0.481919  0.715517   \n",
       "251  0.275930  0.364365  0.302236  0.376615  0.485135  0.627270  0.594828   \n",
       "252  0.785179  0.271804  0.400189  0.457851  0.229526  0.244320  0.396552   \n",
       "253  0.164343  0.571191  0.324472  0.401357  0.249864  0.229873  0.448276   \n",
       "\n",
       "           X8        X9       X10  ...      X466      X467      X468  \\\n",
       "0    0.553781  0.653894  0.375204  ...  0.246376  0.711806  0.008532   \n",
       "1    0.542031  0.447466  0.189233  ...  0.523785  0.760417  0.030930   \n",
       "2    0.516722  0.300371  0.376835  ...  0.185769  0.659722  0.005333   \n",
       "3    0.561615  0.415328  0.313214  ...  0.246376  0.000000  1.000000   \n",
       "4    0.638747  0.660074  0.520392  ...  0.110711  0.517361  0.023677   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "249  0.636336  0.337454  0.432300  ...  0.000000  0.597222  0.016212   \n",
       "250  0.270563  0.407911  0.336052  ...  0.132582  0.975694  0.017065   \n",
       "251  0.435673  0.420272  0.367047  ...  0.246376  0.586806  0.008532   \n",
       "252  0.875565  0.110012  0.337684  ...  0.221768  0.767361  0.024317   \n",
       "253  0.592648  0.498146  0.340946  ...  0.214622  0.621528  0.020904   \n",
       "\n",
       "         X469      X470      X471      X472      X473      X474  Y  \n",
       "0    0.013672  0.008467  0.402240  0.238811  0.274876  0.210238 -1  \n",
       "1    0.033203  0.029759  0.210356  0.309339  0.328358  0.439175 -1  \n",
       "2    0.003906  0.005311  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "3    1.000000  1.000000  0.381877  0.208171  0.208955  0.155761 -1  \n",
       "4    0.022461  0.023447  0.608414  0.212062  0.268657  0.092827 -1  \n",
       "..        ...       ...       ...       ...       ...       ... ..  \n",
       "249  0.013672  0.015997  0.045307  0.147860  0.171642  0.000000  1  \n",
       "250  0.015625  0.016114  0.543689  0.227626  0.320896  0.111165  1  \n",
       "251  0.015625  0.008613  0.343042  0.151751  0.164179  0.143012  1  \n",
       "252  0.019531  0.023461  0.509709  0.398833  0.440299  0.185945  1  \n",
       "253  0.018555  0.020352  0.309061  0.178988  0.201493  0.179953  1  \n",
       "\n",
       "[254 rows x 475 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aca0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y 분할\n",
    "Y=data[\"Y\"].copy()\n",
    "X=data.drop(\"Y\",axis=1)\n",
    "X.head(3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=22,shuffle =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a6bda",
   "metadata": {},
   "source": [
    "[[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)]  \n",
    "**sklearn.model_selection.train_test_split**\n",
    "- **test_size** : float or int, default = 0.25, 정수값일시 test사이즈로 설정하고 싶은 샘플 수 입력\n",
    "- **train_size** : float or int, default = None\n",
    "- **random_state** : int, default = None, 랜덤 seed값 설정, 같은 seed 내에선 동일결과 추출 \n",
    "- **shuffle** : bool, default = True, 데이터셋 무작위 추출, 시계열 데이터와 같이 순차적 추출이 필요한 경우엔 Shuffle = False!\n",
    "- **stratify** : array-like, default = None, True일시 계층적 샘플링 진행 ([참고](https://www.investopedia.com/terms/stratified_random_sampling.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a71bc",
   "metadata": {},
   "source": [
    "### 2. 평가 지표 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cccda2",
   "metadata": {},
   "source": [
    "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)\n",
    "\n",
    "###### 이미지 출처 : https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3227a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 출력 함수\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_all_reg(Y_test,pred):\n",
    "    # Specificity를 구하기 위해 confusion matrix를 이용\n",
    "    cm1 = confusion_matrix(Y_test,pred)\n",
    "    specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    \n",
    "    #결과 검사\n",
    "    #recall = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "    #pre = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "\n",
    "    G_mean = recall_score(Y_test,pred) * specificity1\n",
    "    \n",
    "    print(\"model의 recall 값은 {:.3f}\".format(recall_score(Y_test,pred)))\n",
    "    print(\"model의 2종 오류 확률 값은 {:.3f}\".format(1-recall_score(Y_test,pred)))\n",
    "    print(\"model의 Specificity 값은 {:.3f}\".format(specificity1))\n",
    "    print(\"model의 1종 오류 확률 값은 {:.3f}\".format(1-specificity1))\n",
    "    print(\"model의 precision 값은 {:.3f}\".format(precision_score(Y_test,pred)))\n",
    "    print(\"model의 f1_score 값은 {:.3f}\".format(f1_score(Y_test,pred)))\n",
    "    print(\"model의 G-mean 값은 {:.3f}\".format(np.sqrt(G_mean)))\n",
    "    print(\"model의 accuracy 값은 {:.3f}\".format(accuracy_score(Y_test,pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ec9ce",
   "metadata": {},
   "source": [
    "### 3. 모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f39a4",
   "metadata": {},
   "source": [
    "[[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)]         \n",
    "**주요 하이퍼파라미터** \n",
    "- **loss** : {‘log_loss’, ‘deviance’, ‘exponential’}, default=’log_loss’, 경사하강법을 활용한 예측오류 학습과정에서 사용할 손실함수 \n",
    "- **learning_rate** : float, default=0.1, 학습률 설정\n",
    "    - 너무 작을시 : 예측성능은 높아질 수 있으나, 학습시간이 오래걸리고 local minima 문제가 발생 할 수 있음. 베이스 모델의 영향력이 과도하게 수축되어 과소적합\n",
    "    - 너무 클 시 : 학습속도가 빠르지만 global mimimum값을 찾지 못할 수 있으며, 베이스 모델 영향력이 과도하게 커져 과적합이 나타남.\n",
    "- **n_estimators** : int, default=100, 베이스모델의 개수로 많을수록 예측성능은 높아질 수 있으나 학습시간이 오래걸린다.\n",
    "- **max_depth** : int, default = 3, 각 트리의 최대깊이를 설정. 값이 클수록 모델의 복잡도가 올라간다.\n",
    "- **min_samples_split** : int or float, default = 2, 자식노드를 분할하는데 필요한 최소 샘플의 수\n",
    "- **min_samples_leaf** : int or float, default = 1, leaf node에서 필요한 최소 샘풀수이며, 너무 적을 시 과적합 발생\n",
    "- **max_leaf_nodes** : int, default=None, 최대 leaf node 수 제한\n",
    "- **validation_fraction** : float, default=0.1, 훈련데이터 중 설정 비율만큼 검증용 데이터셋으로 활용한다. \n",
    "- **n_iter_no_change** : int, default=None, validation_fraction에서 설정한 검증데이터셋에서 n_iter_no_change에 지정한 반복 횟수동안 검증점수가 좋아지지 않으면 학습을 조기종료한다.\n",
    "- **ccp_alpha** : non-negative float, default=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a34ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7,shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43894478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002\n",
      "model의 recall 값은 0.795\n",
      "model의 2종 오류 확률 값은 0.205\n",
      "model의 Specificity 값은 0.974\n",
      "model의 1종 오류 확률 값은 0.026\n",
      "model의 precision 값은 0.959\n",
      "model의 f1_score 값은 0.870\n",
      "model의 G-mean 값은 0.880\n",
      "model의 accuracy 값은 0.897\n",
      "\n",
      "model의 recall 값은 0.625\n",
      "model의 2종 오류 확률 값은 0.375\n",
      "model의 Specificity 값은 0.971\n",
      "model의 1종 오류 확률 값은 0.029\n",
      "model의 precision 값은 0.909\n",
      "model의 f1_score 값은 0.741\n",
      "model의 G-mean 값은 0.779\n",
      "model의 accuracy 값은 0.863\n",
      "0.02\n",
      "model의 recall 값은 0.830\n",
      "model의 2종 오류 확률 값은 0.170\n",
      "model의 Specificity 값은 0.974\n",
      "model의 1종 오류 확률 값은 0.026\n",
      "model의 precision 값은 0.961\n",
      "model의 f1_score 값은 0.890\n",
      "model의 G-mean 값은 0.899\n",
      "model의 accuracy 값은 0.911\n",
      "\n",
      "model의 recall 값은 0.625\n",
      "model의 2종 오류 확률 값은 0.375\n",
      "model의 Specificity 값은 0.971\n",
      "model의 1종 오류 확률 값은 0.029\n",
      "model의 precision 값은 0.909\n",
      "model의 f1_score 값은 0.741\n",
      "model의 G-mean 값은 0.779\n",
      "model의 accuracy 값은 0.863\n",
      "0.8\n",
      "model의 recall 값은 0.773\n",
      "model의 2종 오류 확률 값은 0.227\n",
      "model의 Specificity 값은 0.913\n",
      "model의 1종 오류 확률 값은 0.087\n",
      "model의 precision 값은 0.872\n",
      "model의 f1_score 값은 0.819\n",
      "model의 G-mean 값은 0.840\n",
      "model의 accuracy 값은 0.852\n",
      "\n",
      "model의 recall 값은 0.500\n",
      "model의 2종 오류 확률 값은 0.500\n",
      "model의 Specificity 값은 0.914\n",
      "model의 1종 오류 확률 값은 0.086\n",
      "model의 precision 값은 0.727\n",
      "model의 f1_score 값은 0.593\n",
      "model의 G-mean 값은 0.676\n",
      "model의 accuracy 값은 0.784\n"
     ]
    }
   ],
   "source": [
    "#ccp_alpha_list = list(np.arange(0.001,0.02,0.001))\n",
    "#ccp_alpha_list = list(np.arange(0.1,1,0.1))\n",
    "ccp_alpha_list = [0.002,0.02,0.8]\n",
    "\n",
    "train_scores =[]\n",
    "test_scores =[]\n",
    "for alpha in ccp_alpha_list: # 사전에 정의한 리스트의 각 값마다 모델을 학습하여 최적값 도출\n",
    "    print(alpha)\n",
    "    clf = GradientBoostingClassifier(random_state=12,n_estimators = 500,\n",
    "                                     learning_rate = alpha\n",
    "                                     #max_depth=5,learning_rate = alpha\n",
    "                                    #,min_samples_leaf = 2,min_samples_split = 2\n",
    "                                    ,validation_fraction = 0.2\n",
    "                                     ,n_iter_no_change = 10\n",
    "                                     ,ccp_alpha = 0.013 \n",
    "                                   ) # ccp_alpha 값보다 작으면서 비용복잡도가 가장 큰 Subtree로 GBM모델 학습\n",
    "    clf.fit(X_train,Y_train) # 정의한 GBM 모델로 훈련데이터 학습\n",
    "    \n",
    "    preds_train = clf.predict(X_train) # 훈련데이터로 학습한 모델로 y값 예측\n",
    "    preds = clf.predict(X_test) # 실제 테스트 데이터로 y값 예측\n",
    "    \n",
    "    train_scores.append(f1_score(Y_train,preds_train)) # 훈련 성능 평가\n",
    "    test_scores.append(f1_score(Y_test,preds)) # 테스트 성능 평가\n",
    "    print_all_reg(Y_train,preds_train) # 평가지표 출력\n",
    "    print(\"\")\n",
    "    print_all_reg(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc1a5e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"cc\")\n",
    "ax.set_ylabel(\"f1 score\")\n",
    "#ax.set_xticklabels(ccp_alpha_list)\n",
    "\n",
    "#ax.set_title(\"나무 복잡도 증가에 따른 오분류율 그래프\")\n",
    "ax.plot(ccp_alpha_list,train_scores,  label=\"train data\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alpha_list,test_scores,  label=\"test data\", drawstyle=\"steps-post\")\n",
    "#ax.plot(ccp_alpha_list, pd.Series(train_scores).rolling(10,center =True).mean(),  label=\"train data\", drawstyle=\"steps-post\")\n",
    "#ax.plot(ccp_alpha_list, pd.Series(test_scores).rolling(10,center=True).mean(),  label=\"test data\", drawstyle=\"steps-post\")\n",
    "\n",
    "#ax.plot(ccp_alpha_list, train_scores, drawstyle=\"steps-post\")\n",
    "#ax.plot(ccp_alpha_list, test_scores, drawstyle=\"steps-post\")\n",
    "\n",
    "ax.legend()\n",
    "#plt.xlim(0.7)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3104e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e81d91ba51e1bf9900dd7d036cbe3d31d033e3ae1051184964e8f8743fed6bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
