{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cedf23d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a18b6c",
   "metadata": {},
   "source": [
    "- **랜덤포레스트(Random Forest)** 는 **Bagging**기반의 앙상블 기법중 하나로, **Boostrap Sampling**을 통해 여러개의 의사결정나무(베이스모델)를 생성한 후에 다수결 또는 평균에 따라 출력변수를 예측함\n",
    "    - **Bootstrap Sampling** : 학습 데이터로부터 원하는 크기의 샘플을 복원추출하여 샘플데이터를 구축하는 방법\n",
    "    - **Bagging** : Boostrap Aggregating의 약자로, Bootstrap Sampling을 통해 여러개의 표본을 만들고, 각 표본으로부터 베이스 모델을 형성하여 이를 결합하는 앙상블 기법\n",
    "  \n",
    "- 붓스트램 샘플링 뿐만아니라, **무작위 변수선택 기법**을 사용하여 의사결정나무의 다양성을 확보 한다.\n",
    "- **변수의 중요도**를 파악 할 수 있기때문에 공정에서 혐의인자 관리용으로 유용하게 사용 가능한 앙상블 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6452b",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib\n",
    "#한글꺠짐 방지\n",
    "matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c20fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data/class_balance.csv\",encoding=\"EUC-KR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59f8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aca0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y 분할\n",
    "Y=data[\"Y\"].copy()\n",
    "X=data.drop(\"Y\",axis=1)\n",
    "X.head(3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2,shuffle =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdb2ae",
   "metadata": {},
   "source": [
    "[[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)]  \n",
    "**sklearn.model_selection.train_test_split**\n",
    "- **test_size** : float or int, default = 0.25, 정수값일시 test사이즈로 설정하고 싶은 샘플 수 입력\n",
    "- **train_size** : float or int, default = None\n",
    "- **random_state** : int, default = None, 랜덤 seed값 설정, 같은 seed 내에선 동일결과 추출 \n",
    "- **shuffle** : bool, default = True, 데이터셋 무작위 추출, 시계열 데이터와 같이 순차적 추출이 필요한 경우엔 Shuffle = False!\n",
    "- **stratify** : array-like, default = None, True일시 계층적 샘플링 진행 ([참고](https://www.investopedia.com/terms/stratified_random_sampling.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a71bc",
   "metadata": {},
   "source": [
    "### 2. 평가 지표 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544404b",
   "metadata": {},
   "source": [
    "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)\n",
    "\n",
    "###### 이미지 출처 : https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3227a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 출력 함수\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_all_reg(Y_test,pred):\n",
    "    # Specificity를 구하기 위해 confusion matrix를 이용\n",
    "    cm1 = confusion_matrix(Y_test,pred)\n",
    "    specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    \n",
    "    #결과 검사\n",
    "    #recall = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "    #pre = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "\n",
    "    G_mean = recall_score(Y_test,pred) * specificity1\n",
    "    \n",
    "    print(\"model의 recall 값은 {:.3f}\".format(recall_score(Y_test,pred)))\n",
    "    print(\"model의 2종 오류 확률 값은 {:.3f}\".format(1-recall_score(Y_test,pred)))\n",
    "    print(\"model의 Specificity 값은 {:.3f}\".format(specificity1))\n",
    "    print(\"model의 1종 오류 확률 값은 {:.3f}\".format(1-specificity1))\n",
    "    print(\"model의 precision 값은 {:.3f}\".format(precision_score(Y_test,pred)))\n",
    "    print(\"model의 f1_score 값은 {:.3f}\".format(f1_score(Y_test,pred)))\n",
    "    print(\"model의 G-mean 값은 {:.3f}\".format(np.sqrt(G_mean)))\n",
    "    print(\"model의 accuracy 값은 {:.3f}\".format(accuracy_score(Y_test,pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ec9ce",
   "metadata": {},
   "source": [
    "### 3. 모델 학습 및 예측 (하이퍼 파라미터 default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e52025",
   "metadata": {},
   "source": [
    "[[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)]         \n",
    "**주요 하이퍼파라미터** \n",
    "- **n_estimators** : int, default=100, 랜덤포레스트를 구성하는 의사결정 나무 개수 \n",
    "- **criterion** : {\"gini\", \"entorpy\", \"log_loss\"}, default = \"gini\", 클래스 동질성을 측정하는 지표 설정, CART에서는 지니불순도(Gini Impurity)를 사용함.\n",
    "- **max_depth** : int, default = None, 각 트리의 최대깊이를 설정. 값이 클수록 모델의 복잡도가 올라간다.\n",
    "- **min_samples_split** : int or float, default = 2, 자식노드를 분할하는데 필요한 최소 샘플의 수\n",
    "- **min_samples_leaf** : int or float, default = 1, leaf node에서 필요한 최소 샘풀수이며, 너무 적을 시 과적합 발생\n",
    "- **max_leaf_nodes** : int, default=None, 최대 leaf node 수 제한\n",
    "- **max_features** : int, float or {“auto”, “sqrt”, “log2”}, default=sqrt, 각 노드를 분리할 때 사용 할 최대 속성 수\n",
    "- **bootstrap** : bool, default = True, Tree를 형성할때 붓스트램 샘플을 활용. False일시, 중복을 허용하지 않음(=pasting)\n",
    "- **n_jobs** : int, deafult = None, 학습과정에서 사용할 컴퓨터 코어 수, -1로 설정시 가능한 모든 컴퓨터 코어 사용\n",
    "- **random_state** : int, deafult = None, 랜덤값을 지정함으로써 동일결과 출력되도록 설정. default로 사용 시 매번 다른 forest가 형성되나 n_estimators 값이 커질수록 그 변동성은 작아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a34ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model=RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rfc_model.fit(X_train,Y_train)\n",
    "\n",
    "rfc_pred_train = rfc_model.predict(X_train) # 훈련데이터를 활용하여, 학습한 모델로 예측값 생성\n",
    "rfc_pred=rfc_model.predict(X_test) # 테스트 데이터를 활용하여, 학습한 모델로 예측값 생성\n",
    "\n",
    "print_all_reg(Y_train,rfc_pred_train) # 훈련데이터셋을 활용한 모델 성능 평가\n",
    "print(\"\")\n",
    "print_all_reg(Y_test,rfc_pred) # 테스트 데이터셋을 활용한 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731e972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# 데이터 전처리 및 랜덤포레스트 모델 학습과정을 자동화 하는 함수\n",
    "def auto_randomforest(data_x,data_y,model,scaler_how,params):\n",
    "    \n",
    "    # 데이터 split\n",
    "    split_num = 5 # k값 설정\n",
    "    kfold = KFold(n_splits=split_num, shuffle=True, random_state=10) # K-fold 교차검증 적용\n",
    "    score = []\n",
    "    for train_index,test_index in kfold.split(data_x): # k-fold 결과 인덱스가 반환되어 데이터 분할 가능\n",
    "        train_x,valid_x = data_x.iloc[train_index],data_x.iloc[test_index] # data_x를 훈련용, 검증용 데이터셋으로 분할\n",
    "        train_y,valid_y = data_y.iloc[train_index],data_y.iloc[test_index] # data_y를 훈련용, 검증용 데이터셋으로 분할\n",
    "        \n",
    "        # 1 .model select \n",
    "        if model == \"rf\":\n",
    "            \n",
    "            #2. params를 통해 하이퍼 파라미터 세팅\n",
    "            model_use = RandomForestClassifier(random_state=0, n_estimators = params['n_estimators'],\n",
    "                                               max_depth = params[\"max_depth\"],min_samples_leaf = params[\"min_samples_leaf\"],\n",
    "                                               min_samples_split = params[\"min_samples_split\"]\n",
    "                                              )      \n",
    "        else:\n",
    "            print(\"모델이 존재하지 않습니다.\")\n",
    "            break\n",
    "        \n",
    "        # 3. Scaling \n",
    "        if scaler_how == \"minmax\":\n",
    "            print(\"minmax\")\n",
    "            scaler = MinMaxScaler()\n",
    "            train_x = scaler.fit_transform(train_x) # 훈련용 데이터 MinMax Scaler 학습\n",
    "            valid_x = scaler.transform(valid_x) # 앞서 학습한 Scaler로 검증용 데이터 스케일링, 검증 데이터셋에는 fit_transform 하지 않도록 주의\n",
    "        elif scaler_how == \"zscore\":\n",
    "            print(\"z변환\")\n",
    "            scaler2 = StandardScaler()\n",
    "            train_x = scaler2.fit_transform(train_x) # 훈련용 데이터 Standard Scaler 학습\n",
    "            valid_x = scaler2.transform(valid_x) # 앞서 학습한 Scaler로 검증용 데이터 스케일링 검증 데이터셋에는 fit_transform 하지 않도록 주의     \n",
    "        else:\n",
    "            print(\"해당하는 스케일러가 없습니다.\")\n",
    "            #print(\"스케일링 변환 X\")\n",
    "            \n",
    "        # 4. FIT & TEST\n",
    "        model_use.fit(train_x, train_y) # 훈련용 데이터로 랜덤포레스트 모델학습\n",
    "        valid_pred = model_use.predict(valid_x) # 검증용데이터로 예측값 생성\n",
    "        tem = f1_score(valid_pred,valid_y) # 성능평가\n",
    "        score.append(tem) # 결과점수 저장, 해당 과정을 k번 반복\n",
    "    total_score = np.mean(score) # k번 검증된 결과의 평균 출력\n",
    "    print(\"%s 모델 K-fold 결과 f1_score: %f \"%(model,total_score))\n",
    "\n",
    "    return total_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eae864",
   "metadata": {},
   "source": [
    "참고 : [K-fold 교차검증](https://jonsyou.tistory.com/23), [fit_transform과 transform의 차이](https://deepinsight.tistory.com/165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 파라미터 설정 (파라미터를 추가하려면 위의 함수에도 추가해줘야함.)\n",
    "params={\n",
    "    \"n_estimators\":[500,1000],\n",
    "    \"max_depth\":[4,6,8,10],\n",
    "    \"min_samples_leaf\" : [2, 4, 6],\n",
    "    \"min_samples_split\" : range(2,8,2)\n",
    "}\n",
    "\n",
    "# ParameterGrid 통해서 모든 경우의 수 만들기 \n",
    "params_list = list(ParameterGrid(params)) # 총 72가지 경우의수가 담긴 리스트 반환\n",
    "\n",
    "# Grid search 진행\n",
    "score_list = []\n",
    "for params2 in params_list:\n",
    "    tem = auto_randomforest(X_train,Y_train,'rf',\"zscore\",params2) # StandardScaler를 적용하여 랜덤포레스트 모델 학습\n",
    "    score_list.append(tem)\n",
    "\n",
    "# BEST SCORE 계산\n",
    "best_index= np.argmax(score_list)\n",
    "print(\"BEST SCORE\", score_list[best_index])\n",
    "print(\"BEST PARAMS\", params_list[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3363219",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 하이퍼 파라미터로 모델학습\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators = 500,\n",
    "                                               ccp_alpha = 0.04,\n",
    "                                              )  \n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "preds_train = clf.predict(X_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print_all_reg(Y_train,preds_train)\n",
    "print(\" \")\n",
    "print_all_reg(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f53df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#주요 변수 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "var_imp = pd.DataFrame({'var_name':X.columns , 'var_importance': clf.feature_importances_}) # 변수명과 해당 변수의 중요도 값을 데이터프레임 형태로 생성\n",
    "imp_top20=var_imp.sort_values(by=['var_importance'],ascending=False)[:20] # 결과값 내림차순 정렬\n",
    "imp_top20.head(n=10) # 상위값 10개만 출력\n",
    "plt.figure\n",
    "plt.title(\"Feature importaces Top 20\")\n",
    "sns.barplot(x=imp_top20[\"var_importance\"], y=imp_top20[\"var_name\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e81d91ba51e1bf9900dd7d036cbe3d31d033e3ae1051184964e8f8743fed6bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
